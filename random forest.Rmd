---
title: "random forest"
author: "WEITING LIN"
date: "2022-12-07"
output: html_document
---

# Decision tree

    <style type="text/css">
    .main-container {
      max-width: 1800px;
      margin-left: auto;
      margin-right: auto;
    }
    </style>

```{r}
library(tidyverse)
library(dplyr)
predict_data<-read_csv("data_predict.csv")
```

```{r}
str(predict_data)
predict_data$Winner<-as.factor(predict_data$Winner)
```

```{r}
set.seed(2023)
out<-sample(1:nrow(predict_data),size =0.8*nrow(predict_data),replace=F)
test<-predict_data[out,]
train<-predict_data[-out,]
```

```{r}
#install.packages("rpart.plot")
```

```{r,results='hide',echo = T}
library(rpart)
library(rpart.plot)
```

```{r}
fit<-rpart(Winner~.,data=train,method='class')
rpart.plot(fit, extra= 106)
```

After setting model, we turn to check it prediction efficiency. However, its accuracy of prediction is not good at all. So we try random forest and to find if there exits better model.

```{r}
prediction_of_test<-predict(fit,test,method='class')
result=vector()
for(i in 1:nrow(test)){
  if (prediction_of_test[i,1]>prediction_of_test[i,2]){
    result[i]='Blue'
  }else{
    result[i]='Red'
  }
}
output<-table(test$Winner,result)
accuracy<-(sum(diag(output))/sum(output))*100
print(paste0('accuracy rate is ', round(accuracy,2) ,"%"))

```

After using rpart.control(), we found that the model have better prediction ability than the original one.

```{r}
accuracy_tune <- function(fit) {
    predict_unseen <- predict(fit, test, type = 'class')
    table_mat <- table(test$Winner, predict_unseen)
    accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
    accuracy_Test
}

control <- rpart.control(minsplit = 4,
    minbucket = round(5 / 3),
    maxdepth = 3,
    cp = 0)

tune_fit <- rpart(Winner~., data = test, method = 'class', control = control)
accuracy_tune(tune_fit)

```

# Random forest

```{r,result='hide',echo = T}
#install.packages("randomForest")
library(randomForest)
```

```{r}
set.seed(2025)
rf <- randomForest(Winner~., data=train,ntree=1000)
plot(rf)
```

However, the OOB(black line) shows that there is no significant change in error rate with the increasing tree.The result shows that the accuracy rate is 56.9%

```{r,results='hide',echo = T}
library(caret)
```

```{r}
prediction_of_test<-predict(rf,test)
confusionMatrix(prediction_of_test,test$Winner)
```

#### Conclusion

Random forest model is better than decision tree.
